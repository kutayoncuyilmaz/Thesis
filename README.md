Master Thesis


Sentiment classification is a crucial topic in natural language processing. The volume of textual data has increased dramatically with the introduction of the internet into ev- eryday life. As a result, manually analyzing texts becomes unfeasible, and sentiment analysis techniques garner a lot of interest. The use of machine and deep learning mod- els in the field of NLP was a significant moment for the discipline, drastically increas- ing prediction performance. However, the application of transfer learning to NLP tasks and the emergence of pre-trained language models was historic for the discipline, as it enhanced predictive performance while significantly reducing training and inference time. By conducting binary sentiment classification analysis, this thesis seeks to inves- tigate and compare pre-trained language models in order to determine which model is most suited for a given dataset. BERT, XLNet, RoBERTa, and ELECTRA pre-trained language models are used to classify the sentiment of two distinct datasets in terms of text length and domain - the Twitter and IMDB review datasets. Sentiment classi- fication is performed once the core architecture of each model is properly explained. The experiment results show that the XLNet model captures long-term dependencies better than the BERT model, but the BERT model equals the performance of other mod- els on domains with short texts. Among these models, the RoBERTa model performs the best, while the ELECTRA model produces competitive results with substantially shorter training and inference times.

Link to Datasets: https://drive.google.com/drive/folders/18yAK3Gk7s4wZ-alMAG3qqzt1KAfBl9tZ?usp=sharing
